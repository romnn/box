#pragma once

#include <list>

#include "core.hpp"
#include "func_cache.hpp"
#include "gpgpu.hpp"
#include "gpgpu_sim_config.hpp"
#include "memory_space.hpp"
#include "occupancy_stats.hpp"
#include "watchpoint_event.hpp"

class ptx_thread_info;
class ptx_instruction;
class gpgpu_context;
class kernel_info_t;

class gpgpu_sim : public gpgpu_t {
public:
  gpgpu_sim(const gpgpu_sim_config &config, gpgpu_context *ctx);

  // void set_prop(struct cudaDeviceProp *prop);
  //
  // void launch(kernel_info_t *kinfo);
  // bool can_start_kernel();
  // unsigned finished_kernel();
  // void set_kernel_done(kernel_info_t *kernel);
  // void stop_all_running_kernels();
  //
  // void init();
  // void cycle();
  // bool active();

  // bool cycle_insn_cta_max_hit() {
  //   return (m_config.gpu_max_cycle_opt && (gpu_tot_sim_cycle + gpu_sim_cycle)
  //   >=
  //                                             m_config.gpu_max_cycle_opt) ||
  //          (m_config.gpu_max_insn_opt &&
  //           (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt)
  //           ||
  //          (m_config.gpu_max_cta_opt &&
  //           (gpu_tot_issued_cta >= m_config.gpu_max_cta_opt)) ||
  //          (m_config.gpu_max_completed_cta_opt &&
  //           (gpu_completed_cta >= m_config.gpu_max_completed_cta_opt));
  // }
  //
  // void print_stats();
  // void update_stats();
  // void deadlock_check();
  // void inc_completed_cta() { gpu_completed_cta++; }
  // void get_pdom_stack_top_info(unsigned sid, unsigned tid, unsigned *pc,
  //                              unsigned *rpc);
  //
  // int shared_mem_size() const;
  // int shared_mem_per_block() const;
  // int compute_capability_major() const;
  // int compute_capability_minor() const;
  // int num_registers_per_core() const;
  // int num_registers_per_block() const;
  // int wrp_size() const;
  // int shader_clock() const;
  // int max_cta_per_core() const;
  // int get_max_cta(const kernel_info_t &k) const;
  // const struct cudaDeviceProp *get_prop() const;
  // enum divergence_support_t simd_model() const;
  //
  // unsigned threads_per_core() const;
  // bool get_more_cta_left() const;
  // bool kernel_more_cta_left(kernel_info_t *kernel) const;
  // bool hit_max_cta_count() const;
  // kernel_info_t *select_kernel();
  // PowerscalingCoefficients *get_scaling_coeffs();
  // void decrement_kernel_latency();
  //
  const gpgpu_sim_config &get_config() const { return m_config; }
  // void gpu_print_stat();
  // void dump_pipeline(int mask, int s, int m) const;
  //
  // void perf_memcpy_to_gpu(size_t dst_start_addr, size_t count);

  // The next three functions added to be used by the functional simulation
  // function

  //! Get shader core configuration
  /*!
   * Returning the configuration of the shader core, used by the functional
   * simulation only so far
   */
  const shader_core_config *getShaderCoreConfig() { return m_shader_config; }

  //! Get shader core Memory Configuration
  /*!
   * Returning the memory configuration of the shader core, used by the
   * functional simulation only so far
   */
  const memory_config *getMemoryConfig() { return m_memory_config; }

  //! Get shader core SIMT cluster
  /*!
   * Returning the cluster of of the shader core, used by the functional
   * simulation so far
   */
  // simt_core_cluster *getSIMTCluster();

  void hit_watchpoint(unsigned watchpoint_num, ptx_thread_info *thd,
                      const ptx_instruction *pI);

  // backward pointer
  // class gpgpu_context *gpgpu_ctx;

private:
  // clocks
  // void reinit_clock_domains(void);
  // int next_clock_domain(void);
  // void issue_block2core();
  // void print_dram_stats(FILE *fout) const;
  // void shader_print_runtime_stat(FILE *fout);
  // void shader_print_l1_miss_stat(FILE *fout) const;
  // void shader_print_cache_stats(FILE *fout) const;
  // void shader_print_scheduler_stat(FILE *fout, bool print_dynamic_info)
  // const; void visualizer_printstat(); void print_shader_cycle_distro(FILE
  // *fout) const;
  //
  // void gpgpu_debug();

protected:
  // class simt_core_cluster **m_cluster;
  // class memory_partition_unit **m_memory_partition_unit;
  // class memory_sub_partition **m_memory_sub_partition;
  //
  // std::vector<kernel_info_t *> m_running_kernels;
  // unsigned m_last_issued_kernel;
  //
  // std::list<unsigned> m_finished_kernel;
  // // m_total_cta_launched == per-kernel count. gpu_tot_issued_cta == global
  // // count.
  // unsigned long long m_total_cta_launched;
  // unsigned long long gpu_tot_issued_cta;
  // unsigned gpu_completed_cta;
  //
  // unsigned m_last_cluster_issue;
  // float *average_pipeline_duty_cycle;
  // float *active_sms;
  // // time of next rising edge
  // double core_time;
  // double icnt_time;
  // double dram_time;
  // double l2_time;
  //
  // // debug
  // bool gpu_deadlock;
  //
  // //// configuration parameters ////
  const gpgpu_sim_config &m_config;
  //
  // const struct cudaDeviceProp *m_cuda_properties;
  const shader_core_config *m_shader_config;
  const memory_config *m_memory_config;
  //
  // // stats
  // class shader_core_stats *m_shader_stats;
  // class memory_stats_t *m_memory_stats;
  // class power_stat_t *m_power_stats;
  // class gpgpu_sim_wrapper *m_gpgpusim_wrapper;
  // unsigned long long last_gpu_sim_insn;
  //
  // unsigned long long last_liveness_message_time;

  // std::map<std::string, FuncCache> m_special_cache_config;

  //< names of kernel for stat printout
  // std::vector<std::string> m_executed_kernel_names;
  //< uids of kernel launches for stat printout
  // std::vector<unsigned> m_executed_kernel_uids;
  std::map<unsigned, watchpoint_event> g_watchpoint_hits;

  //< format the kernel information
  // std::string executed_kernel_info_string();
  // into a string for stat printout
  // std::string executed_kernel_name();
  //< clear the kernel information after stat printout
  // void clear_executed_kernel_info();

  // virtual void createSIMTCluster() = 0;

public:
  // unsigned long long gpu_sim_insn;
  // unsigned long long gpu_tot_sim_insn;
  // unsigned long long gpu_sim_insn_last_update;
  // unsigned gpu_sim_insn_last_update_sid;
  // occupancy_stats gpu_occupancy;
  // occupancy_stats gpu_tot_occupancy;

  // performance counter for stalls due to congestion.
  // unsigned int gpu_stall_dramfull;
  // unsigned int gpu_stall_icnt2sh;
  // unsigned long long partiton_reqs_in_parallel;
  // unsigned long long partiton_reqs_in_parallel_total;
  // unsigned long long partiton_reqs_in_parallel_util;
  // unsigned long long partiton_reqs_in_parallel_util_total;
  // unsigned long long gpu_sim_cycle_parition_util;
  // unsigned long long gpu_tot_sim_cycle_parition_util;
  // unsigned long long partiton_replys_in_parallel;
  // unsigned long_config(std::string kernel_name);

  // Jin: functional simulation for CDP
private:
  // set by stream operation every time a functoinal simulation is done
  bool m_functional_sim;
  kernel_info_t *m_functional_sim_kernel;

public:
  bool is_functional_sim() { return m_functional_sim; }
  kernel_info_t *get_functional_kernel() { return m_functional_sim_kernel; }
  void functional_launch(kernel_info_t *k) {
    m_functional_sim = true;
    m_functional_sim_kernel = k;
  }
  void finish_functional_sim(kernel_info_t *k) {
    assert(m_functional_sim);
    assert(m_functional_sim_kernel == k);
    m_functional_sim = false;
    m_functional_sim_kernel = NULL;
  }
};

class exec_gpgpu_sim : public gpgpu_sim {
public:
  exec_gpgpu_sim(const gpgpu_sim_config &config, gpgpu_context *ctx)
      : gpgpu_sim(config, ctx) {
    createSIMTCluster();
  }

  virtual void createSIMTCluster();
};
