# This config models the Pascal GP102 (GeForceGTX 1080Ti)
# modified to match the specs of GTX 1080
# see: https://www.es.ele.tue.nl/~heco/courses/ECA/GPU-papers/GeForce_GTX_1080_Whitepaper_FINAL.pdf

# functional simulator specification
-gpgpu_ptx_instruction_classification 0
-gpgpu_ptx_sim_mode 0
-gpgpu_ptx_force_max_capability 61
-gpgpu_ignore_resources_limitation 1

# Device Limits
-gpgpu_stack_size_limit 1024
-gpgpu_heap_size_limit 8388608
-gpgpu_runtime_sync_depth_limit 2
-gpgpu_runtime_pending_launch_count_limit 2048
-gpgpu_kernel_launch_latency 0 # 5000

# Compute capability
-gpgpu_compute_capability_major 6
-gpgpu_compute_capability_minor 1

# SASS execution (only supported with CUDA >= 4.0)
-gpgpu_ptx_convert_to_ptxplus 0
-gpgpu_ptx_save_converted_ptxplus 0

# high level architecture configuration
# edit: GTX 1080 has 20 instead of 28
-gpgpu_n_clusters 28 # 20 for GTX1080
# -gpgpu_n_clusters 1 # 20 for GTX1080
-gpgpu_n_cores_per_cluster 1
# edit: GTX 1080 has 8 instead of 11 memory controllers
-gpgpu_n_mem 12 # 8 for GTX1080
-gpgpu_n_sub_partition_per_mchannel 2  # ROMAN: was 2
# TITAN ADDED: this only affects power calculations
-gpgpu_clock_gated_lanes 1

# Pascal clock domains
#-gpgpu_clock_domains <Core Clock>:<Interconnect Clock>:<L2 Clock>:<DRAM Clock>
# Pascal NVIDIA TITAN X clock domains are adopted from 
# https://en.wikipedia.org/wiki/GeForce_10_series
# edit: GTX 1080 has 1607 base clock and 1733 boost clock
# edit: GTX 1080 has 1251 mem clock
# edit: used same icnt clock as all other configs do that as well (was 2962.0)
# -gpgpu_clock_domains 1607.0:1607.0:1607.0:1251.0 FOR GTX1080
-gpgpu_clock_domains 1417.0:1417.0:1417.0:2500.0

# shader core pipeline config
-gpgpu_shader_registers 65536
-gpgpu_occupancy_sm_number 62

# This implies a maximum of 64 warps/SM
-gpgpu_shader_core_pipeline 2048:32 
-gpgpu_shader_cta 32
-gpgpu_simd_model 1 

# Pipeline widths and number of FUs
# ID_OC_SP, ID_OC_DP, ID_OC_INT, ID_OC_SFU, ID_OC_MEM, OC_EX_SP, OC_EX_DP, OC_EX_INT, OC_EX_SFU, OC_EX_MEM, EX_WB
## Pascal GP102 has 4 SP SIMD units and 4 SFU unit (or 1 SFU)
## There is no INT unit in Pascal
## we need to scale the number of pipeline registers to be equal to the number of SP units
-gpgpu_pipeline_widths 4,0,0,4,4,4,0,0,4,4,8
# -gpgpu_pipeline_widths 4,0,0,1,1,4,0,0,1,1,6 for GTX1080
-gpgpu_num_sp_units 4
-gpgpu_num_sfu_units 4 # 1 for GTX1080?
-gpgpu_tensor_core_avail 0
-gpgpu_num_tensor_core_units 0

# Instruction latencies and initiation intervals
# "ADD,MAX,MUL,MAD,DIV"
# SFU is 32-width in pascal, then dp units initiation is 1 cycle
# GTX1080:
# -ptx_opcode_latency_int 4,13,4,5,145
# -ptx_opcode_initiation_int 1,2,2,2,8
# -ptx_opcode_latency_fp 4,13,4,5,39
# -ptx_opcode_initiation_fp 1,2,1,1,4
# -ptx_opcode_latency_dp 8,19,8,8,330
# -ptx_opcode_initiation_dp 1,2,1,1,130
# TITANX
-ptx_opcode_latency_int 4,13,4,5,145,32
-ptx_opcode_initiation_int 1,1,1,1,4,4
-ptx_opcode_latency_fp 4,13,4,4,39
-ptx_opcode_initiation_fp 1,2,1,1,4
-ptx_opcode_latency_dp 8,19,8,8,330
-ptx_opcode_initiation_dp 8,8,8,8,130
-ptx_opcode_initiation_sfu 4
-ptx_opcode_latency_sfu 20

# L1 data cache
# <nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:**<fifo_entry>
# ** Optional parameter - Required when mshr_type==Texture Fifo
# <mshr> "A" => ASSOC
# <mshr> "S" => SECTOR_ASSOC
# Note: Hashing set index function (H) only applies to a set size of 32 or 64. 
# Pascal GP102 has 96KB Shared memory
# Pascal GP102 has 64KB L1 cache
# EDIT: GTX 1080 has 48 KB L1 instead of 64KB
# (no change here since the original 1080ti config already used 48KB only)
# edit: shared is equal
# -gpgpu_cache:dl1  S:1:128:256,L:L:s:N:L,A:256:8,16:0,32 (RTX3070)
# -gpgpu_cache:dl1  N:64:128:6,L:L:m:N:H,A:128:8,8
# -gpgpu_cache:dl1  S:64:128:6,L:L:m:N:H,A:128:8,8 # BEFORE

-gpgpu_l1_banks 2

# -gpgpu_cache:dl1  S:4:128:48,L:L:m:N:L,A:256:8,16:0,32 # changed H (fermi) to L (linear)
# L=LRU
# B=Write back / L=Local write back global write through
# m=allcate on MISS / s=streaming
# W=write allocate / L=lazy fetch on read
# L=linear set indexing / P=ipoly function
# A=associative mshr
-gpgpu_cache:dl1  S:4:128:48,L:L:m:N:L,A:256:8,16:0,32 # changed H (fermi) to L (linear)
# -gpgpu_cache:dl1  S:4:128:96,L:L:s:N:L,A:256:8,16:0,32 (default for TitanX)
# -gpgpu_cache:dl1PrefL1  S:4:128:96,L:L:s:N:L,A:256:8,16:0,32
# -gpgpu_cache:dl1PrefShared  S:4:128:96,L:L:s:N:L,A:256:8,16:0,32
-gpgpu_shmem_size 98304
-gpgpu_shmem_sizeDefault 98304
-gpgpu_shmem_size_PrefL1 98304
-gpgpu_shmem_size_PrefShared 98304

-gpgpu_smem_latency 24
-gpgpu_flush_l1_cache 1
-gpgpu_flush_l2_cache 0
-gpgpu_l1_cache_write_ratio 0
# The default is NOT to disable the L1 cache, unless cache modifieres is used
# -gmem_skip_L1D 0 is the deprecated option name
-gpgpu_gmem_skip_L1D 0
-icnt_flit_size 40
-gpgpu_n_cluster_ejection_buffer_size 32
-gpgpu_l1_latency 82

# 64 sets, each 128 bytes 16-way for each memory sub partition (128 KB per memory sub partition).
# This gives 3MB L2 cache
# EDIT: GTX 1080 has 2MB L2 instead of 3MB (no change here since 8 instead of 11 mem controllers)
# -gpgpu_cache:dl2 N:64:128:16,L:B:m:W:L,S:1024:1024,4:0,32 # used to be 128:4
# 128B cache line * 64sets * 16ways * 8mem_ctrl * 2sub_part_per_mem_ctrl = 2097152
# -gpgpu_cache:dl2 S:64:128:16,L:B:m:W:L,A:1024:1024,4:0,32 # used to be 128:4
# {<nsets>:<bsize>:<assoc>,<replacement>:<write_policy>:<alloc_policy>:<write_alloc_policy>:<set index function>,<mshr_type>:<mshr_entries>:<mshr_merge>,<miss_queue>,<result fifo>,<data port>}
# L=LRU
# B=Write back
# m=allcate on MISS
# W=write allocate / L=lazy fetch on read
# L=linear set indexing / P=ipoly function
# A=associative mshr
-gpgpu_cache:dl2 S:64:128:16,L:B:m:W:L,A:256:64,16:0,32 # used to be 128:4
# -gpgpu_cache:dl2 S:128:128:16,L:B:m:W:L,A:1024:1024,4:0,32 # used to be 128:4
-gpgpu_cache:dl2_texture_only 0 
# fill L2 on memcopy
-gpgpu_perf_sim_memcpy 1

# edit: GTX 1080 specs for this not found but assumed equal
# 4 KB Inst.
-gpgpu_cache:il1 N:8:128:4,L:R:f:N:L,S:2:48,4
-gpgpu_inst_fetch_throughput 8
# -gpgpu_inst_fetch_throughput 1
# -gpgpu_cache:il1 N:8:128:4,L:R:f:N:L,A:2:48,4
# 48 KB Tex 
# -gpgpu_tex_cache:l1 N:16:128:24,L:R:m:N:L,F:128:4,128:2 # BEFORE
-gpgpu_tex_cache:l1 N:4:128:48,L:R:m:N:L,F:128:4,128:2
# 12 KB Const
-gpgpu_const_cache:l1 N:128:64:2,L:R:f:N:L,A:2:64,4

# temp workaround: l1 const cache (readonly) does not support sectors at the moment
-gpgpu_perfect_inst_const_cache 1

# GTX1080 enable operand collector 
## larger operand collectors and reg_banks are needed for the 4 warp schedulers and 4 SIMD units
# -gpgpu_operand_collector_num_units_sp 20
# -gpgpu_operand_collector_num_units_sfu 4
# -gpgpu_operand_collector_num_units_mem 8
# -gpgpu_operand_collector_num_in_ports_sp 4
# -gpgpu_operand_collector_num_out_ports_sp 4
# -gpgpu_operand_collector_num_in_ports_sfu 1
# -gpgpu_operand_collector_num_out_ports_sfu 1
# -gpgpu_operand_collector_num_in_ports_mem 1
# -gpgpu_operand_collector_num_out_ports_mem 1

# in sub_core_model, schedulers are isolated, each scheduler has its own register file and EUs
-gpgpu_sub_core_model 1
# use generic operand collectors instead of specialized operand collectors
-gpgpu_enable_specialized_operand_collector 0
-gpgpu_operand_collector_num_units_gen 8
-gpgpu_operand_collector_num_in_ports_gen 8
-gpgpu_operand_collector_num_out_ports_gen 8

# 16 register banks, 4 banks per subcore
-gpgpu_num_reg_banks 16 # 32 for GTX1080
# ADDED for Titan
-gpgpu_reg_file_port_throughput 2

# shared memory bankconflict detection 
-gpgpu_shmem_num_banks 32
-gpgpu_shmem_limited_broadcast 0
-gpgpu_shmem_warp_parts 2 # ROMAN: was 1
-gpgpu_coalesce_arch 61 # ROMAN

# interconnection
-network_mode 1 
# -inter_config_file config_fermi_islip.icnt
-inter_config_file config_pascal_islip.icnt

# memory partition latency config 
# -gpgpu_l2_rop_latency 1 # was 120
# -dram_latency 1 # was 100
-gpgpu_l2_rop_latency 210 # was 120
-dram_latency 190 # was 100

# dram model config
-gpgpu_dram_scheduler 1 # 0=FIFO 1=FRFCFS
# The DRAM return queue and the scheduler queue together should provide buffer
# to sustain the memory level parallelism to tolerate DRAM latency 
# To allow 100% DRAM utility, there should at least be enough buffer to sustain
# the minimum DRAM latency (100 core cycles).  I.e. 
#   Total buffer space required = 100 x 924MHz / 700MHz = 132
# GTX 1080
# -gpgpu_frfcfs_dram_sched_queue_size 64
# -gpgpu_dram_return_queue_size 116
-gpgpu_frfcfs_dram_sched_queue_size 64
-gpgpu_dram_return_queue_size 64
-gpgpu_dram_partition_queues 32:32:32:32

# for NVIDIA GeForceGTX 1080Ti, bus width is 352bits (11 DRAM chips x 32 bits)
# 11 memory paritions, 4 bytes (1 DRAM chip) per memory partition
# the atom size of GDDR5X (the smallest read request) is 32 bytes 
# edit: GTX 1080 has 256 bus width but no change here required
-gpgpu_n_mem_per_ctrlr 1
-gpgpu_dram_buswidth 4
-gpgpu_dram_burst_length 8
-dram_data_command_freq_ratio 4  # GDDR5X is QDR
-gpgpu_mem_address_mask 1
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS


# Use the same GDDR5 timing from hynix H5GQ1H24AFR
# GTX1080
# disable bank groups for now, set nbkgrp to 1 and tCCDL and tRTPL to 0
# -gpgpu_dram_timing_opt "nbk=16:CCD=2:RRD=6:RCD=12:RAS=28:RP=12:RC=40:
#                         CL=12:WL=4:CDLR=5:WR=12:nbkgrp=1:CCDL=0:RTPL=0"
# Use the same GDDR5 timing, scaled to 2500MHZ
-gpgpu_dram_timing_opt "nbk=16:CCD=2:RRD=8:RCD=16:RAS=37:RP=16:RC=52:
                        CL=16:WL=6:CDLR=7:WR=16:nbkgrp=4:CCDL=4:RTPL=3"
-dram_bnk_indexing_policy 0 # 0=normal 1=xoring with higher bits
-dram_bnkgrp_indexing_policy 1 #0=higher bits 1=lower bits

# Pascal 102 has four schedulers per core
-gpgpu_num_sched_per_core 4
# Two Level Scheduler with active and pending pools
#-gpgpu_scheduler two_level_active:6:0:1
# Loose round robbin scheduler
#-gpgpu_scheduler lrr
# Greedy then oldest scheduler
-gpgpu_scheduler gto

## In Pascal, a warp scheduler can issue 2 insts per cycle using 2 diff execution units
-gpgpu_max_insn_issue_per_warp 2
-gpgpu_dual_issue_diff_exec_units 1

# stat collection
-gpgpu_memlatency_stat 14 
-gpgpu_runtime_stat 500
-enable_ptx_file_line_stats 1
-visualizer_enabled 0

# power model configs
-power_simulation_enabled 0
# -power_simulation_enabled 1
# -gpuwattch_xml_file gpuwattch_gtx1080Ti.xml
# -accelwattch_xml_file gpuwattch_gtx1080Ti.xml

# tracing functionality
#-trace_enabled 1
#-trace_components WARP_SCHEDULER,SCOREBOARD
#-trace_sampling_core 0

